<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI vs. NYT Connections - Analysis</title>
    <link rel="stylesheet" href="../css/style.css" />
    <link rel="icon" type="image/png" href="../favicon.png" />
  </head>
  <body>
    <div class="container">
      <nav class="topbar">
        <a href="/">Home</a>
        <a href="/projects-and-notes.html">Projects & Notes</a>
      </nav>
      <div class="main-content">
        <article class="writeup-container">
          <header>
            <h4>
              Running a Custom Benchmark: AI vs. The New York Times'
              <span class="italic">Connections</span>
            </h4>
            <p class="results-link">
              <a
                href="../projects-and-notes/ai-vs-connections-results/index.html"
                target="_blank"
                rel="noopener noreferrer"
                >View results</a
              >
            </p>
            <p class="date">March 18, 2025</p>
          </header>
          <section>
            <p>
              After the release of OpenAI’s
              <a
                href="https://openai.com/o1/"
                target="_blank"
                rel="noopener noreferrer"
                >o1 model</a
              >
              in December 2024, I became curious how the model would perform on
              <span class="italic">The New York Times</span>
              <span class="italic">
                <a
                  href="https://www.nytimes.com/games/connections"
                  target="_blank"
                  rel="noopener noreferrer"
                  >Connections</a
                >
              </span>
              puzzle. My initial experiments through the chat interface were
              mostly successful, although the model did make mistakes
              occasionally. Among many examples it was impressive to see it
              correctly work out that ‘Lincoln’, ‘Biscuit’, ‘Corn’, and
              ‘Stained’ are
              <span class="italic">Misspelled Words in Famous Rock Bands</span>.
            </p>
            <p>
              Some quick Googling led me to
              <a
                href="https://futurism.com/the-byte/openai-o1-nyt-connections"
                target="_blank"
                rel="noopener noreferrer"
                >several</a
              >
              <a
                href="https://mindmatters.ai/2025/01/large-language-models-llms-flunk-word-game-connections/"
                target="_blank"
                rel="noopener noreferrer"
                >articles</a
              >
              documenting failed experiments with o1 and
              <span class="italic">Connections</span>, which got me thinking
              about undertaking a more structured attempt to evaluate the
              performance of various OpenAI models on the puzzle.
            </p>
            <p>
              I built a web application that allowed me to input the 16 daily
              <span class="italic">Connections</span> words and send them, along
              with the puzzle’s rules, to different OpenAI models for solving.
              Initially, I tested three models: GPT-4o, o1-mini, and o1
              (o1-preview). Just as I was finalizing the experiment in February
              2025, OpenAI released GPT-4.5 and gave me API access to o3-mini.
              Curious to see how these models would perform on the puzzle, I
              added them to the experiment.
            </p>
            <p>
              For each puzzle, I evaluated each of these models in two ‘modes’:
              (1)
              <span class="italic">
                <span class="italic">Single Pass</span>
              </span>
              , where the model attempted to group all 16 words into 4 sets at
              once, and (2)
              <span class="italic">Four at a Time</span>, where the model
              followed standard <span class="italic">Connections</span>
              rules, guessing 4 words per round and receiving feedback
              (“correct,” “incorrect,” “one away”, or “already guessed”), with
              up to 4 mistakes allowed.
            </p>
          </section>
          <section>
            <h4>Results</h4>
            <p>
              I ran the experiment on 30 daily
              <span class="italic">Connections</span> puzzles between January
              and February 2025. The chart below shows a summary of the results.
              The full results are available
              <a
                href="../projects-and-notes/ai-vs-connections-results/index.html"
                target="_blank"
                rel="noopener noreferrer"
                >here</a
              >.
            </p>
            <figure>
              <img
                src="../assets/results.png"
                alt="experiment results - performance"
                class="results-img"
              />
            </figure>
            <p>
              In short, o1 is almost always able to solve the puzzle. In the
              harder
              <span class="italic">Single Pass</span> mode, it identified all
              four groupings correctly 83% of the time. Under standard
              <span class="italic">Connections</span> rules, it succeeded 100%
              of the time, more than half of the time (57%) without a single
              mistake.
            </p>
            <p>
              The second-best performer was o3-mini, which solved the puzzle 47%
              of the time in <span class="italic">Single Pass</span> mode and
              90% of the time in
              <span class="italic">Four at a Time</span> mode.
            </p>
            <p>
              The other models struggled with the puzzle to varying degrees.
              o1-mini performed decently in
              <span class="italic">Four at a Time</span> mode, solving the
              puzzle 50% of the time in that mode. Despite not being a reasoning
              model, GPT-4.5 had comparable performance to o1-mini, and was
              clearly superior to GPT-4o. This aligns with OpenAI
              <a
                href="https://openai.com/index/introducing-gpt-4-5/"
                target="_blank"
                rel="noopener noreferrer"
                >findings</a
              >:
            </p>
            <p class="quote">
              …we provide GPT‑4.5’s results on standard academic benchmarks to
              illustrate its current performance on tasks traditionally
              associated with reasoning. Even by purely scaling up unsupervised
              learning, GPT‑4.5 shows meaningful improvements over previous
              models like GPT‑4o.
            </p>
            <section>
              <h4>Performance vs. Cost</h4>
              <figure>
                <img
                  src="../assets/cost.png"
                  alt="experiment results - cost"
                  class="results-img"
                />
              </figure>
              <p>
                Despite its effectiveness, o1 has two notable drawbacks, the
                first being cost. The model is expensive to use, with high token
                costs and heavy token consumption. The average cost per correct
                response from o1 was $0.29 (<span class="italic"
                  >Single Pass</span
                >) and $0.86 (<span class="italic">Four at a Time</span>). For
                comparison, a correct response from the next best overall
                performer, o3-mini, was just $0.02 (<span class="italic"
                  >Single Pass</span
                >) and $0.07 (<span class="italic">Four at a Time</span>).
              </p>

              <p>
                The second drawback is time. Although I didn’t record how long
                each puzzle took to solve—in hindsight, this would have been
                useful—o1 was generally very slow, sometimes taking several
                minutes to work through a puzzle. This was especially noticeable
                in
                <span class="italic">Four at a Time</span> mode, where there
                were multiple exchanges between the interface and the API.
                o1-mini was also noticeably slow, while o3-mini was consistently
                faster.
              </p>
              <p>
                Overall, o3-mini is a compelling alternative to o1, balancing
                strong performance with affordability and speed.
              </p>
            </section>
            <section>
              <h4>Consistency</h4>
              <p>
                Since each model was tested on a puzzle only once, one concern I
                had was the possibility that the observed results were one-offs.
                I became curious: How consistent is model performance on a given
                puzzle? To explore this, I conducted a consistency evaluation,
                running each of the 5 models on a subset of 10 puzzles 10 times
                (in each mode). The results can be viewed by toggling to
                <span class="italic">Consistency Test</span> on the
                <a
                  href="../projects-and-notes/ai-vs-connections-results/index.html"
                  target="_blank"
                  rel="noopener noreferrer"
                  >results page</a
                >.
              </p>
              <p>
                Overall, o1 is a highly consistent model. Across 100 runs (10
                runs per puzzle on 10 puzzles) in
                <span class="italic">Four at a Time</span> mode, the model
                failed to solve a puzzle just once. It also showed high
                consistency in <span class="italic">Single Pass</span> mode: It
                solved a puzzle 80% of the time or more in 60% of the sample,
                and struggled to solve the two puzzles in the sample it failed
                to solve on its initial run, solving them just 20% and 30% of
                the time. That said, while o1 is
                <span class="italic">highly</span> consistent in
                <span class="italic">Single Pass</span> mode, it is not
                <span class="italic">perfectly</span> consistent—meaning it can
                occasionally fail to solve a puzzle it is generally able to
                solve, and vice versa.
              </p>
              <p>
                The same applies to the other models: While they are broadly
                consistent—most of the time, they can either solve a puzzle or
                not—they are not perfectly consistent. A model can occasionally
                surprise us by solving a puzzle it usually fails at, or by
                failing a puzzle it otherwise reliably solves.
              </p>
            </section>
          </section>
          <section>
            <h4>API</h4>
            <p>
              OpenAI’s API implementation differs depending on the model being
              used. The most significant difference in this experiment was that
              o1-mini and o1 do not support structured outputs—meaning they
              cannot generate JSON responses that strictly follow a predefined
              schema. This was essential for the solver interface, particularly
              in
              <span class="italic">Four at a Time</span> mode, where the model’s
              guess needed to be validated and evaluated for the solving
              sequence to continue. A simple workaround was to ask the o1 models
              to format their responses in a JSON-like structure, which was then
              passed to GPT-4o for conversion into schema-compliant output.
            </p>
            <p>
              Another small difference is that, unlike the other models, the
              o1-series models do not accept a ‘system’ message. This message
              provides broad instructions to guide model behavior in an API
              interaction; in the application, it's where I provided the models
              with the game's rules. It’s unclear why the o1-series models lack
              this feature. When interacting with them in the application, I
              ended up including the game instructions in the initial ‘user’
              message instead.
            </p>
          </section>
          <section>
            <h4>Other thoughts</h4>
            <p>
              Although it feels like AI model improvements are quickly becoming
              routine, I still find it impressive and fun to watch strong models
              like o1 work through and solve a
              <span class="italic">Connections</span> puzzle. I still get
              tempted to run the daily puzzle through the solver to see o1 solve
              it—or maybe finally fail.
            </p>
            <p>
              For fun, I recorded the solver in action one last time on today’s
              puzzle:
            </p>
            <figure class="video-container">
              <video
                src="../assets/o1-preview.mp4"
                controls
                class="video"
              ></video>
              <figcaption class="fig-caption">
                The solver interface: o1 solving the
                <span class="italic">Connections</span>
                puzzle from March 18, 2025. Notice the model's slowness
                (fast-forward to 1:10 to see its first response).
              </figcaption>
            </figure>
            <p>
              I think the experiment clearly shows that OpenAI’s advanced
              reasoning models <span class="italic">can</span> solve
              <span class="italic">Connections</span>. Moreover, the failure of
              non-reasoning models (like GPT-4o) to solve the puzzle underscores
              that reasoning/chain-of-thought models are fundamentally better at
              solving logic problems.
            </p>
            <p>
              Although we shouldn’t assume that these findings generalize to
              other contexts, the superior performance of all models in
              <span class="italic">Four at a Time</span> mode suggests that
              decomposing problems into smaller steps and providing AI models
              with feedback can improve their performance.
            </p>
            <p>
              With the exception of o1 in
              <span class="italic">Four at a Time</span> mode, inconsistent
              model performance is a nagging issue. For example, out of 10
              puzzles tested for consistency with o1 in
              <span class="italic">Single Pass</span> mode, 4 puzzles were
              solved only 70-80% of the time—that is, the model solved them
              <span class="italic">most</span> of the time but not always. This
              presents a problem: a model that usually produces the correct
              result, but occasionally stumbles. Once again, while we can’t
              necessarily generalize from this experiment to other contexts,
              this suggests that even tried-and-tested AI models should not be
              relied on uncritically.
            </p>
            <p>
              In terms of next steps, I’m curious how models from other
              companies/providers would perform on the puzzle compared to
              OpenAI. This might be coming up.
            </p>
          </section>
        </article>
      </div>
    </div>
  </body>
</html>
