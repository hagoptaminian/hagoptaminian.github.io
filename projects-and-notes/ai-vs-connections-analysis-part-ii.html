<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI vs. NYT Connections - Analysis</title>
    <link rel="stylesheet" href="../css/style.css" />
    <link rel="icon" type="image/png" href="../favicon.png" />
  </head>
  <body>
    <div class="container">
      <nav class="topbar">
        <a href="/">Home</a>
        <a href="/projects-and-notes.html">Projects & Notes</a>
      </nav>
      <div class="main-content">
        <article class="writeup-container">
          <header>
            <h4>
              Running a Custom Benchmark: AI vs. The New York Times'
              <span class="italic">Connections</span> (Part II)
            </h4>
            <p class="results-link">
              <a
                href="../projects-and-notes/ai-vs-connections-results/index.html"
                target="_blank"
                rel="noopener noreferrer"
                >View results</a
              >
            </p>
            <p class="date">May 26, 2025</p>
          </header>
          <section>
            <p>
              Earlier this year, I built an application to programmatically
              evaluate how different OpenAI models perform on the
              <span class="italic">New York Times</span> word game
              <span class="italic">Connections</span> . I was mainly interested
              in how OpenAI’s then-newly released ‘reasoning’ model, o1, would
              perform on the puzzle. Although o1 has since been superseded by a
              better, cheaper, and faster model (o3), and is scheduled to be
              deprecated in the OpenAI API, it performed extremely well on
              <span class="italic">Connections</span>.
            </p>
            <p>
              Since completing the OpenAI evaluation, I’ve been wanting to
              expand the experiment to include other providers, partly out of
              curiosity about how their models would perform on the puzzle, and
              partly as a way to learn more about their models and APIs.
            </p>
            <p>
              I ended up testing the following additional providers and models:
              (1) Google (Gemini 2.0 Flash, 2.5 Flash, and 2.5 Pro), (2)
              Anthropic (Claude 3.0 Opus, 3.5 Haiku, 3.5 Sonnet, and 3.7
              Sonnet), (3) DeepSeek (DeepSeek Chat and DeepSeek Reasoner), and
              (4) Mistral (Mistral Medium 3 and Mistral Large). Consistent with
              the original experiment, I tested each of the models in ‘Single
              Pass’ and ‘Four at a Time’ modes on each of the puzzles in the
              sample.
            </p>
          </section>
          <section>
            <h4>A few thoughts on the LLM landscape</h4>
            <p>
              Many people have noted their frustration with how AI companies are
              naming their models. In general, it’s impossible to tell what a
              model might be good for from its name. For example, Google has a
              model called Gemini 2.5 Pro (a ‘thinking model’) and another
              called Gemini 2.5 Flash—for which you ‘can configure a thinking
              budget’. How is Gemini 2.5 Flash with a thinking budget different
              from Gemini 2.5 Pro?
            </p>
            <p>
              Even worse, model descriptions are being rendered meaningless by
              AI marketing superlatives. For example, here’s how Anthropic
              describes the four models I used in this experiment:
            </p>
            <ul>
              <li>
                Claude 3.7 Sonnet: Highest level of intelligence and capability
                with toggleable extended thinking
              </li>
              <li>
                Claude 3.5 Sonnet: High level of intelligence and capability
              </li>
              <li>Claude 3.5 Haiku: Intelligence at blazing speeds</li>
              <li>
                Claude 3.0 Opus: Powerful model for complex tasks with top-level
                intelligence
              </li>
            </ul>
            <p>
              It’s impossible to figure out what these descriptions mean in
              practice. If all of the models have ‘intelligence’, the label
              ceases to have significance.
            </p>
            <p>
              A further complication comes from the fact that many providers now
              offer models based on a ‘chain-of-thought’ paradigm, whereby a
              model undertakes an extended ‘test-time’ reasoning process before
              providing its response. Generally, these models are being called
              ‘reasoning’ or ‘thinking’ models, but it doesn’t look like this
              convention holds across providers. For example, Mistral describes
              its latest model, Mistral Large, as capable of ‘top-tier reasoning
              for high-complexity tasks and sophisticated problems’, a
              description which led me to assume that this must be Mistral’s
              version of a reasoning model. Upon testing the model, however,
              this turned out not to be the case.
            </p>
            <p>
              A final issue is the speed with which models are being released
              and then replaced with newer versions. For example, in the two
              months between running the initial experiment and this extended
              one, OpenAI announced the replacement and deprecation of three of
              the models I tested initially (o1-preview, o1-mini, and
              GPT-4.5)—models which were themselves introduced only 3-6 months
              ago.
            </p>
          </section>
          <section>
            <h4>Upshot: Things will take time to settle</h4>
            <p>
              The lack of clarity and churn are understandable. Models are
              improving quickly, so frequent upgrades are inevitable. At the
              same time, we as users are still figuring out the differences
              between the models and learning about the various model paradigms.
              The more we use these models, and the more familiar we become with
              their range of capabilities, the more we’ll be able to match the
              right model to the right task.
            </p>
          </section>
          <section>
            <h4>Results</h4>
            <p>
              Given the somewhat large number of models tested, I won’t go
              through the results for each provider in detail. In the analysis
              below, I compare the performance of each provider’s flagship
              reasoning model, or, when this is not available, the most recent
              version of its strongest model. In addition to o1-preview from the
              initial experiment, these models are: Gemini 2.5 Pro (Google),
              Claude 3.7 Sonnet (with ‘Extended Thinking’ enabled with a budget
              of 16,000 tokens), DeepSeek Reasoner, and Mistral Large.
            </p>
            <figure>
              <img
                src="../assets/part-ii-results.png"
                alt="experiment results"
                class="results-2-img"
              />
            </figure>
            <p>
              None of the models matched o1-preview’s performance on the puzzle.
              Google’s Gemini 2.5 Pro came closest, solving the puzzle 83% of
              the time in <span class="italic">Four at a Time</span> mode. It
              was followed by DeepSeek Reasoner and Claude 3.7 Sonnet, which
              solved the puzzle roughly 60% of the time in
              <span class="italic">Four at a Time</span> mode. Mistral Large
              scored only 13%, which is understandable given that the model does
              not incorporate a chain-of-thought process.
            </p>
            <figure>
              <img
                src="../assets/part-ii-cost.png"
                alt="experiment results (cost)"
                class="results-2-img"
              />
            </figure>
            <p>
              In terms of cost, the non-OpenAI models models are significantly
              cheaper than o1-preview, with average cost per correct response
              ranging from $0.04 to $0.22 in
              <span class="italic">Four at a Time</span> mode, compared to $0.86
              for o1-preview. Perhaps this comparison is slightly unfair: o3,
              OpenAI’s newer reasoning model, is cheaper than o1, and may also
              be more efficient (hence consuming fewer tokens). Additionally,
              o3’s ‘mini’ version (o3-mini), which I did test in the initial
              experiment, performed better than Gemini 2.5 Pro (90% score in
              <span class="italic">Four at a Time</span>), and was also cheaper
              ($0.07 per correct response).
            </p>
            <p>
              Finally, I measured response latency in this part of the
              experiment:
            </p>
            <figure>
              <img
                src="../assets/part-ii-latency.png"
                alt="experiment results (latency)"
                class="results-2-img"
              />
            </figure>
            <p>
              Although I didn’t capture latency in the initial part of the
              experiment, my feeling is that Gemini 2.5 Pro and Claude 3.7
              Sonnet had roughly similar performance on speed as o1-preview. I
              was surprised how slow DeepSeek Reasoner was, sometimes taking up
              to 30 minutes to go through a
              <span class="italic">Four at a Time</span> run. My understanding
              is that this slowness has more to do with the China-based
              deployment I accessed rather than something internal to the model.
            </p>
          </section>
          <section>
            <h4>Final thoughts</h4>
            <p>
              Obviously, <span class="italic">Connections</span> is an arbitrary
              benchmark. When I first started this experiment with the OpenAI
              models, I had two motivations: (1) A genuine curiosity about how
              o1 would perform on the puzzle over a decent sample, (2) A desire
              to figure out the OpenAI API and code to implement a puzzle
              solving sequence, especially in
              <span class="italic">Four at a Time</span> mode—not something too
              complex, but not trivial either, especially if one is working with
              the API and structured outputs for the first time.
            </p>
            <p>
              Having done this in the first part of the experiment, the novelty
              had worn off somewhat when I started this part. I did have some
              marginal curiosity about how models from other providers would
              perform compared to o1: If performance was roughly similar, it
              would perhaps suggest a greater degree of model commoditization.
              Given the results, I do wonder why some reasoning models (o1 and
              Gemini 2.5 Pro) performed better on the puzzle than others:
              Perhaps they are currently genuinely better/smarter.
            </p>
          </section>
        </article>
      </div>
    </div>
  </body>
</html>
